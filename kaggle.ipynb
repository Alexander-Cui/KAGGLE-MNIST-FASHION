{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import os\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Pre_process():\n",
    "    \n",
    "    def __init__ (self, size):\n",
    "        self.IMG_SIZE = size\n",
    "\n",
    "    #Should keep track of count. Note: BE WEARY OF BALANCE WHEN USING DIFFERENT DATASET\n",
    "    \n",
    "    #dir_path holds path to either test or train folder\n",
    "    #Receives an img_list and copies and appends to this list.\n",
    "    def load_img(self, img_list, dir_path):\n",
    "        \n",
    "        list_copy = img_list.copy() #don't want to leak reference\n",
    "        \n",
    "        for label_name in tnrange(os.listdir(dir_path)): #for each label in directory\n",
    "            for img_name in tqdm_notebook(os.listdir( os.path.join (dir_path,label_name))): #read img into list\n",
    "                img_path = os.path.join(dir_path+\"\\\\\"+label_name+\"\\\\\" + img_name)\n",
    "                img = cv2.imread(img_path,0) #Grey scale since color doesnt change the letter\n",
    "                img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE)) #So all images are same size\n",
    "                list_copy.append([np.array(img),label_name]) #MIGHT NEED TO CONVERT INTO ONE HOT VECTOR\n",
    "        \n",
    "        np.random.shuffle(list_copy)\n",
    "        return list_copy\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 62\n",
    "path_ASL_alphabet_train = R\"D:\\Github\\MAIS202\\AmericanSignLanguage-Translator\\Images\\asl-alphabet\\asl_alphabet_train\\asl_alphabet_train\"\n",
    "path_ASL_alphabet_test = R\"D:\\Github\\MAIS202\\AmericanSignLanguage-Translator\\Images\\asl-alphabet\\asl_alphabet_test\\asl_alphabet_test\"\n",
    "\n",
    "LOAD_IMG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "pre_pro_dataset = Pre_process(IMG_SIZE)\n",
    "\n",
    "if LOAD_IMG:\n",
    "    train = pre_pro_dataset.load_img(train, path_ASL_alphabet_train)\n",
    "    np.save(\"train_dataset\", train)\n",
    "#     test = pre_pro_dataset.load_img(test, path_ASL_alphabet_test)\n",
    "#     np.save(\"test_dataset\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tqdm' from 'c:\\\\users\\\\alexa\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\site-packages\\\\tqdm\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class CNN_Model(nn.Module):\n",
    "    \n",
    "    def init(self):\n",
    "        super().__init__()    #decided i did not need padding since corners of images do not seem important for my case\n",
    "        self.conv1 = nn.conv2d (1, 32, 3, 1) #input:1 (greyscale is 1 channel), output: 64 layer conv, kernal size: 3x3, step:1\n",
    "        self.conv2 = nn.conv2d (32,64,3,1,1) \n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.conv2d (64,64,3,1) #Preserving size + corners\n",
    "        self.conv4 = nn.conv2d (64,64,3,1,1) \n",
    "        self.conv5 = nn.conv2d (64,64,3,1,1) \n",
    "        \n",
    "        self.conv6 = nn.conv2d (64,128,3,1)\n",
    "        self.conv7 = nn.conv2d (128,256 , 3,1)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.linear ()\n",
    "        self.fc2 = nn.linear\n",
    "        \n",
    "        def forward (self, x):\n",
    "            x = F.relu(self.conv1(x)) #use relu activation function #image size is now 60\n",
    "            x = F.maxpool (x, 2,2) #max pool size 2x2, image size now 30\n",
    "            x = F.relu (self.conv2(x)) #size 28\n",
    "            x = F.maxpool (x,2,2) #size is 14\n",
    "            \n",
    "            x = F.relu(self.conv3(x))\n",
    "            x = F.relu(self.conv4(x))\n",
    "            x = F.relu(self.conv5(x))\n",
    "            \n",
    "            x = F.relu (self.conv6(x)) #size is 12\n",
    "            x = F.maxpool(x,2,2) # size is 6\n",
    "            x = F.relu(self.conv7(x)) #size is 4\n",
    "            x = x.view (-1, 4*4*256)\n",
    "            x = nn.fc1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
